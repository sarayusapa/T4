{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NgxbJAKeqSP1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_data():\n",
        "    with open('shakespeare-2.txt', mode='r', encoding='utf-8') as f:\n",
        "        data = f.read()  # data is a string that contains the text file\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_data()\n",
        "words = data.split()\n",
        "distinct_words = sorted(list(set(words)))  # vocabulary\n",
        "word_to_idx = dict((word, i) for i, word in enumerate(distinct_words))  # each word has an index\n",
        "idx_to_word = dict((i, word) for i, word in enumerate(distinct_words))  # each index has a word. useful for text generation\n"
      ],
      "metadata": {
        "id": "wIjeqIv-qlzD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "N_seq = 50  # Length of the input sequence to be fed\n",
        "N_words = len(words)\n",
        "N_vocab = len(distinct_words)\n",
        "print(N_words, N_vocab)\n"
      ],
      "metadata": {
        "id": "6mFfUc4rqquI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fce05b3-f2b3-4cb5-d930-c800353b8b22"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18582 5235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "for i in range(0, N_words - N_seq, 1):\n",
        "    # Given x of 50 words (Input Sequence), predict the next word y (Conditional Probability)\n",
        "    x = words[i:i+N_seq]\n",
        "    y = words[i+N_seq]\n",
        "    x_train.append([word_to_idx[x_i] for x_i in x])\n",
        "    y_train.append(word_to_idx[y])\n",
        "\n",
        "m = len(x_train)\n",
        "assert m == len(y_train), \"Length mismatch error\"\n"
      ],
      "metadata": {
        "id": "f4vO8nx5qxFE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Convert x_train (list of lists of word indices) directly to numpy array of integers\n",
        "x_train = np.array(x_train, dtype=np.int64)  # shape (m, N_seq)\n",
        "\n",
        "# Convert y_train (list of word indices) directly to numpy array of integers\n",
        "y_train = np.array(y_train, dtype=np.int64)  # shape (m,)\n",
        "\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_layers=3):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # shape: (batch, seq_len, embedding_dim)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Example instantiation:\n",
        "embedding_dim = 128\n",
        "hidden_size = 512\n",
        "model = LSTMModel(vocab_size=N_vocab, embedding_dim=embedding_dim, hidden_size=hidden_size, output_size=N_vocab)\n"
      ],
      "metadata": {
        "id": "Xfg8vfDvqzFs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming `model` is your instantiated PyTorch model\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()  # expects raw logits and class indices as targets\n",
        "\n",
        "# Since y_train is already integer indices (not one-hot), no need to convert\n",
        "# So you can directly use y_train as targets during training\n",
        "\n",
        "# Make sure your model forward returns raw logits (no softmax)\n",
        "# e.g. in model.forward():\n",
        "#   return out  # no softmax applied here\n"
      ],
      "metadata": {
        "id": "juObXIPPrB-c"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "PATH_SAVE = \"shakespearean_generator_2.pth\"  # PyTorch convention for saved models\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, loss, path=PATH_SAVE):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, path)\n"
      ],
      "metadata": {
        "id": "rFry-YdLrZ-X"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "\n",
        "# Convert numpy arrays to tensors\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.long)  # integer indices for embedding lookup\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # already integer class labels\n",
        "\n",
        "# Create DataLoader for batching\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "num_epochs = 30\n",
        "best_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)  # inputs shape: (batch, seq_len), outputs: (batch, num_classes)\n",
        "        loss = criterion(outputs, labels)  # labels are class indices\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint if loss improved\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        save_checkpoint(model, optimizer, epoch, best_loss)\n"
      ],
      "metadata": {
        "id": "esslakyhrcGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "317801d7-3af1-4d5c-98e9-3540593a50a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 - Loss: 7.4962\n",
            "Epoch 2/30 - Loss: 7.1304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Generation"
      ],
      "metadata": {
        "id": "CRwgLzkTr4tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(seed_words, N_words):\n",
        "    \"\"\"\n",
        "    seed_words: list of initial words (strings)\n",
        "    N_words: number of new words to generate\n",
        "    \"\"\"\n",
        "    model.eval()  # set to evaluation mode\n",
        "\n",
        "    # Convert seed words to indices\n",
        "    x0 = [word_to_idx[word] for word in seed_words]\n",
        "    generated_indices = x0.copy()\n",
        "\n",
        "    for _ in range(N_words):\n",
        "        # Prepare input as tensor of shape (1, N_seq)\n",
        "        x_tensor = torch.tensor([x0], dtype=torch.long)  # batch size 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(x_tensor)  # raw logits, shape (1, N_vocab)\n",
        "            probs = F.softmax(logits, dim=1).cpu().numpy().ravel()\n",
        "\n",
        "        # Sample next word index from probability distribution\n",
        "        idx = np.random.choice(N_vocab, p=probs)\n",
        "\n",
        "        generated_indices.append(idx)\n",
        "\n",
        "        # Slide the window: drop first word, append new word idx\n",
        "        x0 = x0[1:] + [idx]\n",
        "\n",
        "    generated_words = [idx_to_word[i] for i in generated_indices]\n",
        "    return ' '.join(generated_words)\n"
      ],
      "metadata": {
        "id": "ywtmSLerr8KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_seed = \"your awesome character is very powerful today\".lower()\n",
        "seed_words = initial_seed.split()\n",
        "\n",
        "# Ensure all words are in the vocabulary\n",
        "words_input = set(seed_words)\n",
        "words_valid = set(word_to_idx.keys())\n",
        "invalid_words = words_input.difference(words_valid)\n",
        "if invalid_words:\n",
        "    raise SyntaxError(f\"Input contains invalid words: {invalid_words}\")\n",
        "\n",
        "# Truncate long sequences\n",
        "if len(seed_words) > N_seq:\n",
        "    seed_words = seed_words[-N_seq:]  # keep the last N_seq words\n",
        "\n",
        "# Pad short sequences with a special token or just ' ' (space)\n",
        "N_pad = max(N_seq - len(seed_words), 0)\n",
        "\n",
        "# Check if <PAD> token exists; if not, use space or another token in vocab\n",
        "pad_token = '<PAD>' if '<PAD>' in word_to_idx else ' '\n",
        "\n",
        "seed_words = [pad_token] * N_pad + seed_words\n",
        "\n",
        "print(\"The seed words are:\", seed_words)\n"
      ],
      "metadata": {
        "id": "29sYBWrTsFZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = [word_to_idx[word] for word in seed_words]"
      ],
      "metadata": {
        "id": "NkEvQZ_wsIwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_sentence = generate(seed_words, 500)[N_pad:]  # Remove the prepended padding, if any"
      ],
      "metadata": {
        "id": "DCMQ4IWjsM0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_sentence = ' '.join([idx_to_word[i] for i in generated_sentence])\n",
        "print(generated_sentence)"
      ],
      "metadata": {
        "id": "bl_zic-ysPDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'shakespeare_final.pth')"
      ],
      "metadata": {
        "id": "-P3447sssRpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMModel(input_size, hidden_size, output_size, num_layers=3)\n",
        "model.load_state_dict(torch.load('shakespeare_final.pth'))\n",
        "model.eval()  # set to evaluation mode"
      ],
      "metadata": {
        "id": "m0KPoRdosTc0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}